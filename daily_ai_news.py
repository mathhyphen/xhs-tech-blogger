#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XHS AIæ—¥æŠ¥ç”Ÿæˆå™¨ v2.0 - OpenClaw Browserç‰ˆ
ä¸€é”®è·å–ä»Šæ—¥AIåŠ¨æ€å¹¶ç”Ÿæˆå°çº¢ä¹¦æ–‡ç« 

Usage:
    python daily_ai_news.py              # ç”Ÿæˆæ—¥æŠ¥
    python daily_ai_news.py --publish    # ç”Ÿæˆå¹¶å‡†å¤‡å‘å¸ƒ
    python daily_ai_news.py --dry-run    # æµ‹è¯•æ¨¡å¼ï¼Œä¸ä¿å­˜
"""

import subprocess
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict

class XHSAIDailyPublisher:
    """å°çº¢ä¹¦AIæ—¥æŠ¥å‘å¸ƒå™¨"""
    
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.news_data = []
        
        # å¤„ç†è¾“å‡ºç›®å½•ï¼ˆæ”¯æŒç»å¯¹è·¯å¾„ï¼‰
        output_config = self.config.get('output', {})
        save_dir = output_config.get('save_directory', 'output')
        
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ç›¸å¯¹äºè„šæœ¬ç›®å½•
        if Path(save_dir).is_absolute():
            self.output_dir = Path(save_dir)
        else:
            self.output_dir = Path(__file__).parent / save_dir
        
        self.output_dir.mkdir(exist_ok=True)
    
    def _load_config(self, config_path: str = None) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not config_path:
            config_path = Path(__file__).parent / 'config.json'
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[Warning] æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
            return self._default_config()
    
    def _default_config(self) -> dict:
        """é»˜è®¤é…ç½®"""
        return {
            'news_sources': {
                'ai_news_collectors': {'enabled': True},
                'news_aggregator': {'enabled': True},
                'techmeme': {'enabled': True}
            },
            'xiaohongshu': {'enabled': True},
            'output': {'save_directory': 'output'}
        }
    
    def _run_openclaw_skill(self, skill_name: str, timeout: int = 120) -> str:
        """è¿è¡ŒOpenClaw skill"""
        try:
            result = subprocess.run(
                ['npx', 'openclaw', 'skills', 'run', skill_name],
                capture_output=True,
                text=True,
                shell=True,
                timeout=timeout
            )
            return result.stdout + result.stderr
        except subprocess.TimeoutExpired:
            return f"[Timeout] Skill {skill_name} è¿è¡Œè¶…æ—¶"
        except Exception as e:
            return f"[Error] {e}"
    
    def collect_from_ai_news_collectors(self) -> List[Dict]:
        """ä»ai-news-collectorsæ”¶é›†æ–°é—»"""
        print("[1/3] æ­£åœ¨è¿è¡Œ ai-news-collectors...")
        
        if not self.config.get('news_sources', {}).get('ai_news_collectors', {}).get('enabled'):
            print("      [Skip] æœªå¯ç”¨")
            return []
        
        output = self._run_openclaw_skill('ai-news-collectors', timeout=180)
        
        # è§£æè¾“å‡º
        news_list = self._parse_ai_news_output(output)
        print(f"      [OK] æ”¶é›†åˆ° {len(news_list)} æ¡")
        return news_list
    
    def collect_from_news_aggregator(self) -> List[Dict]:
        """ä»news-aggregator-skill-2æ”¶é›†æ–°é—»"""
        print("[2/3] æ­£åœ¨è¿è¡Œ news-aggregator-skill-2...")
        
        if not self.config.get('news_sources', {}).get('news_aggregator', {}).get('enabled'):
            print("      [Skip] æœªå¯ç”¨")
            return []
        
        # æ„å»ºå…³é”®è¯
        keywords = self.config.get('news_sources', {}).get('news_aggregator', {}).get(
            'keywords', ['AI', 'LLM', 'GPT', 'OpenAI']
        )
        keyword_str = ','.join(keywords)
        
        try:
            # è¿è¡Œfetch_newsè„šæœ¬
            skill_path = Path.home() / '.openclaw' / 'workspace' / 'skills' / 'news-aggregator-skill-2'
            result = subprocess.run(
                ['python', 'scripts/fetch_news.py', 
                 '--source', 'all',
                 '--limit', '10',
                 '--keyword', keyword_str],
                capture_output=True,
                text=True,
                shell=True,
                timeout=120,
                cwd=str(skill_path)
            )
            
            # è§£æJSONè¾“å‡º
            try:
                data = json.loads(result.stdout)
                news_list = self._parse_news_aggregator_output(data)
                print(f"      [OK] æ”¶é›†åˆ° {len(news_list)} æ¡")
                return news_list
            except:
                print(f"      [Warning] è§£æå¤±è´¥")
                return []
                
        except Exception as e:
            print(f"      [Error] {e}")
            return []
    
    def collect_from_techmeme(self) -> List[Dict]:
        """ä½¿ç”¨OpenClaw Browserä»TechMemeæ”¶é›†AIæ–°é—»"""
        print("[3/3] æ­£åœ¨ä» TechMeme æ”¶é›†...")
        
        if not self.config.get('news_sources', {}).get('techmeme', {}).get('enabled'):
            print("      [Skip] æœªå¯ç”¨")
            return []
        
        try:
            # ä½¿ç”¨openclaw browserè®¿é—®TechMeme
            subprocess.run(
                ['openclaw', 'browser', 'navigate', 'https://www.techmeme.com'],
                timeout=30
            )
            
            # æ‰§è¡ŒJavaScriptæå–æ–°é—»
            js_code = """
            (function() {
                const articles = document.querySelectorAll('div.hentry');
                const results = [];
                const aiKeywords = ['AI', 'artificial intelligence', 'ChatGPT', 'OpenAI', 
                                   'LLM', 'machine learning', 'Claude', 'model'];
                
                for (let article of articles.slice(0, 15)) {
                    const titleEl = article.querySelector('div.hed');
                    const sourceEl = article.querySelector('div.by');
                    
                    if (titleEl) {
                        const title = titleEl.innerText.trim();
                        const isAI = aiKeywords.some(kw => title.toLowerCase().includes(kw.toLowerCase()));
                        
                        if (isAI) {
                            results.push({
                                title: title,
                                source: sourceEl ? 'TechMeme - ' + sourceEl.innerText.trim() : 'TechMeme'
                            });
                        }
                    }
                }
                
                return JSON.stringify(results);
            })()
            """
            
            result = subprocess.run(
                ['openclaw', 'browser', 'evaluate', '--fn', js_code],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # è§£æç»“æœ
            try:
                data = json.loads(result.stdout)
                news_list = []
                for item in data:
                    news_list.append({
                        'title': item['title'],
                        'source': item['source'],
                        'url': 'https://www.techmeme.com',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'source_type': 'techmeme'
                    })
                print(f"      [OK] æ”¶é›†åˆ° {len(news_list)} æ¡")
                return news_list
            except:
                print(f"      [Warning] è§£æå¤±è´¥")
                return []
                
        except Exception as e:
            print(f"      [Error] {e}")
            return []
    
    def _parse_ai_news_output(self, output: str) -> List[Dict]:
        """è§£æai-news-collectorsè¾“å‡º"""
        news_list = []
        lines = output.split('\n')
        current_news = {}
        
        for line in lines:
            if line.strip().startswith('**') and line.strip().endswith('**'):
                if current_news:
                    news_list.append(current_news)
                current_news = {
                    'title': line.strip().strip('*'),
                    'source': 'AI News Collectors',
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'source_type': 'ai_news_collectors'
                }
            elif 'http' in line and current_news:
                current_news['url'] = line.strip()
        
        if current_news:
            news_list.append(current_news)
        
        return news_list
    
    def _parse_news_aggregator_output(self, data: dict) -> List[Dict]:
        """è§£ænews-aggregatorè¾“å‡º"""
        news_list = []
        
        if isinstance(data, list):
            for item in data:
                news_list.append({
                    'title': item.get('title', ''),
                    'source': item.get('source', 'News Aggregator'),
                    'url': item.get('url', ''),
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'source_type': 'news_aggregator'
                })
        
        return news_list
    
    def deduplicate_and_rank(self, news_list: List[Dict]) -> List[Dict]:
        """å»é‡å¹¶æ’åº"""
        print("[æ±‡æ€»] æ­£åœ¨å»é‡å’Œæ’åº...")
        
        # å»é‡
        seen = set()
        unique_news = []
        
        for news in news_list:
            key = news.get('title', '')[:20].lower()
            if key and key not in seen:
                seen.add(key)
                unique_news.append(news)
        
        # é™åˆ¶æ•°é‡
        final_news = unique_news[:10]
        
        # æ·»åŠ emoji
        emojis = ['ğŸ¯', 'ğŸ“ˆ', 'ğŸ¦', 'ğŸ’°', 'ğŸ¬', 'âš–ï¸', 'âš¡', 'ğŸ“¹', 'ğŸ”’', 'ğŸ—ï¸']
        for i, news in enumerate(final_news):
            news['emoji'] = emojis[i % len(emojis)]
        
        print(f"       å»é‡å: {len(final_news)} æ¡")
        return final_news
    
    def generate_xhs_content(self, news_list: List[Dict]) -> str:
        """ç”Ÿæˆå°çº¢ä¹¦æ ¼å¼å†…å®¹"""
        today = datetime.now()
        count = len(news_list)
        
        # æ ‡é¢˜
        title_template = self.config.get('xiaohongshu', {}).get('post_format', {}).get(
            'title_template', 'æ˜¨æ—¥AIåœˆ{count}å¤§çƒ­ç‚¹'
        )
        title = title_template.format(
            date=today.strftime('%mæœˆ%dæ—¥'),
            count=count
        )
        
        # å¤´éƒ¨
        header_template = self.config.get('xiaohongshu', {}).get('post_format', {}).get(
            'header', '{date} AIåœˆçœŸå®çƒ­ç‚¹'
        )
        header = header_template.format(date=today.strftime('%Yå¹´%mæœˆ%dæ—¥'))
        
        # æ­£æ–‡
        content_lines = [f'æ ‡é¢˜ï¼š{title}', '', header, 'ï¼ˆæ¥æºï¼šå¤šæºèšåˆï¼Œå·²å»é‡ï¼‰', '', 'æ˜¨å¤©AIåœˆå‘ç”Ÿäº†ä»€ä¹ˆå¤§äº‹ï¼Ÿ', 'æˆ‘æ•´ç†äº†æœ€çƒ­èµ„è®¯', '']
        
        for i, news in enumerate(news_list, 1):
            content_lines.append(f"{i}. {news['emoji']} {news['title']}")
            if news.get('summary'):
                content_lines.append(f"   {news['summary']}")
            content_lines.append(f"   æ¥æºï¼š{news['source']}")
            content_lines.append(f"   é“¾æ¥ï¼š{news['url']}")
            content_lines.append('')
        
        # å°¾éƒ¨
        content_lines.extend([
            'â€”â€”',
            'æ–°é—»æ¥æºï¼šå¤šæºèšåˆï¼ˆå·²å»é‡ï¼‰',
            'ä½ æœ€å…³æ³¨å“ªä¸€æ¡ï¼Ÿè¯„è®ºåŒºèŠèŠ',
            'å…³æ³¨æˆ‘ï¼Œæ¯å¤©AIçƒ­ç‚¹ä¸é”™è¿‡',
            '',
            '#AI #äººå·¥æ™ºèƒ½ #ç§‘æŠ€çƒ­ç‚¹ #OpenAI'
        ])
        
        return '\n'.join(content_lines)
    
    def save_content(self, content: str) -> Path:
        """ä¿å­˜å†…å®¹"""
        today = datetime.now().strftime('%Y%m%d')
        filename = f"xhs_ai_news_{today}.txt"
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return filepath
    
    def run(self, dry_run: bool = False) -> tuple:
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("=" * 70)
        print("XHS AIæ—¥æŠ¥ç”Ÿæˆå™¨ v2.0")
        print("=" * 70)
        print()
        
        # æ”¶é›†æ–°é—»
        all_news = []
        all_news.extend(self.collect_from_ai_news_collectors())
        all_news.extend(self.collect_from_news_aggregator())
        all_news.extend(self.collect_from_techmeme())
        
        print()
        print(f"[æ±‡æ€»] å…±æ”¶é›† {len(all_news)} æ¡åŸå§‹æ–°é—»")
        
        if not all_news:
            print("[Error] æœªæ”¶é›†åˆ°ä»»ä½•æ–°é—»ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œskillé…ç½®")
            return None, None
        
        # å»é‡æ’åº
        final_news = self.deduplicate_and_rank(all_news)
        
        # ç”Ÿæˆå†…å®¹
        content = self.generate_xhs_content(final_news)
        
        # ä¿å­˜
        if not dry_run:
            filepath = self.save_content(content)
            print()
            print("=" * 70)
            print(f"ç”Ÿæˆå®Œæˆï¼")
            print(f"æ–‡ä»¶: {filepath}")
            print(f"æ–°é—»æ•°: {len(final_news)} æ¡")
            print("=" * 70)
        else:
            print()
            print("[Dry Run] æµ‹è¯•æ¨¡å¼ï¼Œæœªä¿å­˜æ–‡ä»¶")
            filepath = None
        
        return content, filepath

def main():
    parser = argparse.ArgumentParser(description='XHS AIæ—¥æŠ¥ç”Ÿæˆå™¨')
    parser.add_argument('--publish', action='store_true', help='ç”Ÿæˆåå‡†å¤‡å‘å¸ƒåˆ°å°çº¢ä¹¦')
    parser.add_argument('--dry-run', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼Œä¸ä¿å­˜æ–‡ä»¶')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå‘å¸ƒå™¨
    publisher = XHSAIDailyPublisher(config_path=args.config)
    
    # è¿è¡Œ
    content, filepath = publisher.run(dry_run=args.dry_run)
    
    if content and args.publish:
        print()
        print("å‡†å¤‡å‘å¸ƒåˆ°å°çº¢ä¹¦...")
        print("è¿è¡Œ: python xhs_auto_publish.py --latest")

if __name__ == '__main__':
    main()
